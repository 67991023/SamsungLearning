{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "k7pdmZCbJ23m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ULJVTHu6hIo"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "ds = pd.read_json(\"hf://datasets/Amod/mental_health_counseling_conversations/combined_dataset.json\", lines=True)\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "device = \"cuda\" #Metal Performance Shaders\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\") #padding_side show where padding tokens should be added to sequences\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                            torch_dtype=torch.bfloat16,\n",
        "                                            device_map=device)"
      ],
      "metadata": {
        "id": "GpN5y694GDO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_pipeline = pipeline(task=\"text-generation\", #pipeline เชื่อมต่อโมเดลกับขั้นตอนอื่น ๆ\n",
        "                               model=model, tokenizer=tokenizer)\n",
        "generation_pipeline(\"Hello\", max_new_tokens=25)"
      ],
      "metadata": {
        "id": "6eHiQjtMG9mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"hello how are you?\",\n",
        "    \"The nigger\"\n",
        "]\n",
        "\n",
        "tokenized = tokenizer(prompts,padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(tokenized)\n",
        "print(tokenized[\"input_ids\"].shape)"
      ],
      "metadata": {
        "id": "0A1vknsMBsGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokenized[\"input_ids\"])"
      ],
      "metadata": {
        "id": "24aLqejTE61e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized[\"attention_mask\"] #tell which have value"
      ],
      "metadata": {
        "id": "Rfs_WYnAFHb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = [\n",
        "    {\n",
        "      \"role\" : \"system\",\n",
        "      \"content\" : \"You are a smart AI assistant\"\n",
        "    },\n",
        "    {\n",
        "        \"role\" : \"user\",\n",
        "        \"content\" : \"Where does the sun rises?\"\n",
        "    },\n",
        "    {\n",
        "        \"role\" : \"assistant\",\n",
        "        \"content\" : \"Aye aye\"\n",
        "    }\n",
        "]\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenized = tokenizer.apply_chat_template( #converts prompt from chat message format to a single string sequence\n",
        "    prompt_template,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True, #if set True, will get number that easily insert to model\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(tokenized)"
      ],
      "metadata": {
        "id": "AW4cUuYQFkpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(tokenized.to(device), max_new_tokens=30)"
      ],
      "metadata": {
        "id": "y4UKv3wRDPSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tokenizer.batch_decode(out)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "4hTzeeLxBqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "bbOxHF7tN-M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict next word"
      ],
      "metadata": {
        "id": "FC_WWZV22B9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " sentence = \"you are game\"\n",
        " input_ids = tokenizer([sentence], return_tensors=\"pt\")[\"input_ids\"].to(device) # return_tensors use to change type to tensor from list\n",
        "\n",
        " out = model(input_ids = input_ids) # model(input) does one step of generation by outputting the probabilities of next word\n",
        " out"
      ],
      "metadata": {
        "id": "YadrUgNxIkhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict what is the next word\n",
        "probability_dist = nn.Softmax(dim=-1)(out.logits[0, -1]) # softmax change to probability\n",
        "print(out.logits)\n",
        "print(\"_____________________________\")\n",
        "print(out.logits.shape) # models store millions of subwords, vocab has 128256 tokens\n",
        "print(\"_____________________________\")\n",
        "print(input_ids)\n",
        "print(\"_____________________________\")\n",
        "print(probability_dist)\n",
        "print(\"_____________________________\")\n",
        "print(out.logits.argmax(), out.logits.argmax(axis=-1)) #argmax find maximum value in the list\n",
        "print(tokenizer.decode(out.logits.argmax(axis=-1)[0, -1]))"
      ],
      "metadata": {
        "id": "XciZrCQdLoYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss function (training on sequence)"
      ],
      "metadata": {
        "id": "T5moXm3iaRf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4aaCTNNhefLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\"use teng service instead.\"]\n",
        "tokenized = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(tokenized)\n",
        "tokenized2 = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"] #print only input_ids\n",
        "print(tokenized2)\n",
        "print(tokenizer.batch_decode(tokenized2))"
      ],
      "metadata": {
        "id": "ewXM5dXANWUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenized[\"input_ids\"][:, :-1]\n",
        "target_ids = tokenized[\"input_ids\"][:, 1:]\n",
        "\n",
        "print(\"Input Seq: \", input_ids)\n",
        "print(\"Target Seq: \", tokenizer.batch_decode(input_ids))\n",
        "print(target_ids)\n",
        "print(tokenizer.batch_decode(target_ids))"
      ],
      "metadata": {
        "id": "Z03Uik9ogUeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques = \"what service should I use?\"\n",
        "answer = \"Teng Service\"\n",
        "\n",
        "prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"what service should I use?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Service:\"}\n",
        "]\n",
        "answer = \"Teng Service\"\n",
        "\n",
        "chat_template = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    continue_final_message=True,\n",
        "    tokenize=False\n",
        ")\n",
        "print(chat_template)"
      ],
      "metadata": {
        "id": "Jo21HFcPl7tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_response = chat_template + \" \" + answer + tokenizer.eos_token\n",
        "print(full_response)"
      ],
      "metadata": {
        "id": "bC85_Zxct9gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(full_response, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
        "print(tokenized)\n",
        "print(tokenizer.batch_decode(tokenized))"
      ],
      "metadata": {
        "id": "R8vTrIbgvU_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenized[:, :-1]\n",
        "target_ids = tokenized[:, 1:]\n",
        "\n",
        "print(\"Input Seq: \", input_ids)\n",
        "print(\"Target Seq: \", tokenizer.batch_decode(input_ids))\n",
        "print(target_ids)\n",
        "print(tokenizer.batch_decode(target_ids))"
      ],
      "metadata": {
        "id": "JONbiApzwR8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstrate of using tokenizer\n",
        "print(tokenizer.convert_ids_to_tokens(32346))"
      ],
      "metadata": {
        "id": "xQH9DTM2xGNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_tokenized = tokenizer([\" \" + answer], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"] # answer in prompt\n",
        "print(labels_tokenized)\n",
        "print(tokenizer.batch_decode(labels_tokenized))"
      ],
      "metadata": {
        "id": "kvkKl_zdx4MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape"
      ],
      "metadata": {
        "id": "CqfRAsVS1bNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_tokenized2 = tokenizer([\" \" + answer + tokenizer.eos_token], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=target_ids.shape[1])[\"input_ids\"]\n",
        "print(labels_tokenized2)"
      ],
      "metadata": {
        "id": "9ZCF_57c0w5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_tokenized_fixed = torch.where(labels_tokenized2 != tokenizer.pad_token_id, labels_tokenized2, -100)\n",
        "labels_tokenized_fixed"
      ],
      "metadata": {
        "id": "h7kyHlGCy2zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Assuming torch is used for tensor operations\n",
        "\n",
        "def generate_input_output_pair(prompt, target_responses):\n",
        "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
        "    full_response_text = [\n",
        "        chat_template + \" \" + target_response + tokenizer.eos_token\n",
        "        for chat_template, target_response in zip(chat_templates, target_responses)\n",
        "    ]\n",
        "\n",
        "    input_ids_tokenized = tokenizer(full_response_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    labels_tokenized = tokenizer([\n",
        "        \"\" + response + tokenizer.eos_token for response in target_responses\n",
        "    ], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=input_ids_tokenized.shape[1])[\"input_ids\"]\n",
        "\n",
        "    labels_tokenized_fixed = torch.where(labels_tokenized == tokenizer.pad_token_id, labels_tokenized, -100)\n",
        "    labels_tokenized_fixed[:, -1] = tokenizer.pad_token_id\n",
        "\n",
        "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
        "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
        "\n",
        "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
        "\n",
        "    return input_ids_tokenized_left_shifted, labels_tokenized_right_shifted, attention_mask"
      ],
      "metadata": {
        "id": "NE88meII2U3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Assuming torch is used for tensor operations\n",
        "\n",
        "def generate_input_output_pair_test(prompt, target_responses):\n",
        "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
        "    full_response_text = [\n",
        "        chat_template + \" \" + target_response + tokenizer.eos_token\n",
        "        for chat_template, target_response in zip(chat_templates, target_responses)\n",
        "    ]\n",
        "\n",
        "    input_ids_tokenized = tokenizer(full_response_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    labels_tokenized = tokenizer([\n",
        "        \"\" + response + tokenizer.eos_token for response in target_responses\n",
        "    ], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=input_ids_tokenized.shape[1])[\"input_ids\"]\n",
        "\n",
        "    # The original code was marking pad tokens in labels as -100, but also the last token\n",
        "    # Let's revisit the logic for labels to ensure correct loss calculation.\n",
        "    # For causal language modeling, the labels are typically the input IDs shifted by one position.\n",
        "    # We only want to calculate the loss for the tokens corresponding to the target response.\n",
        "    # Tokens before the target response and padding tokens should be masked with -100.\n",
        "\n",
        "    # Create labels by shifting input_ids_tokenized\n",
        "    labels = input_ids_tokenized.clone()\n",
        "    # Shift labels to the left (the target for index i is the token at index i+1)\n",
        "    labels[:, :-1] = input_ids_tokenized[:, 1:]\n",
        "    # The last label should be -100 or the pad token id depending on desired behavior\n",
        "    # For now, let's set it to -100 to match the original masking intent for non-response tokens\n",
        "    labels[:, -1] = -100 # Assuming the last token should not contribute to the loss calculation for the label sequence\n",
        "\n",
        "    # Create a mask to identify the tokens corresponding to the target response in the labels\n",
        "    # This requires knowing where the target response starts in the full response text.\n",
        "    # A more robust approach might be to tokenized the target response separately\n",
        "    # and use its tokenized length to mask the labels tensor.\n",
        "\n",
        "    # Let's try masking based on the tokenized target response length\n",
        "    target_response_tokenized = tokenizer([response + tokenizer.eos_token for response in target_responses], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
        "    target_response_length = target_response_tokenized.shape[1]\n",
        "\n",
        "    # Mask the labels tensor. We want to calculate loss only for the target response tokens.\n",
        "    # The tokens corresponding to the prompt and the continuation in chat_template should be masked.\n",
        "    # The number of prompt tokens + continuation tokens is input_ids_tokenized.shape[1] - target_response_length\n",
        "    prompt_and_continuation_length = input_ids_tokenized.shape[1] - target_response_length\n",
        "    labels[:, :prompt_and_continuation_length] = -100 # Mask the prompt and continuation\n",
        "\n",
        "    # Mask padding tokens in labels as -100\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "\n",
        "    # Ensure attention mask is correctly shaped for the shifted input\n",
        "    attention_mask = input_ids_tokenized[:, :-1] != tokenizer.pad_token_id\n",
        "\n",
        "    # Return a dictionary\n",
        "    return {\n",
        "        \"input_ids\": input_ids_tokenized[:, :-1], # Input is the sequence up to the second to last token\n",
        "        \"labels\": labels[:, 1:], # Labels are the sequence from the second token onwards (shifted)\n",
        "        \"attention_mask\": attention_mask\n",
        "    }\n",
        "\n",
        "# The training loop code remains the same after this change\n",
        "# data = generate_input_output_pair(prompt=[training_prompt], target_responses=[target_response])\n",
        "# data[\"input_ids\"] = data[\"input_ids\"].to(device)\n",
        "# data[\"labels\"] = data[\"labels\"].to(device)\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "# for i in range(10):\n",
        "#   out = model(input_ids=data[\"input_ids\"].to(device))\n",
        "#   loss = calculate_loss(out.logits, data[\"labels\"]).mean()\n",
        "\n",
        "#   loss.backward()\n",
        "#   optimizer.step()\n",
        "#   optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "QE_u7gGD1I9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_input_output_pair(\n",
        "    prompt = [\n",
        "        [\n",
        "          {\"role\": \"user\", \"content\": \"what service should I use?\"},\n",
        "          {\"role\": \"assistant\", \"content\": \"Service:\"}\n",
        "        ]\n",
        "    ],\n",
        "    target_responses = [\"teng service\"]\n",
        ")\n",
        "\n",
        "out = model(input_ids=data[0].to(device))\n",
        "out"
      ],
      "metadata": {
        "id": "977mtOEJGnvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0])\n",
        "print(data[0].shape)\n",
        "print(data[1])\n",
        "print(data[1].shape)\n",
        "print(data[2])\n",
        "print(data[2].shape)"
      ],
      "metadata": {
        "id": "in-Kl60qIEnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(logits, labels):\n",
        "  loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "  cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "  return cross_entropy_loss"
      ],
      "metadata": {
        "id": "8ANvNPR-KAy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.logits.shape"
      ],
      "metadata": {
        "id": "apIQOt7cNYG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_loss(out.logits, data[1].to(device))"
      ],
      "metadata": {
        "id": "R7qx9mSMODWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\", \"content\": \"Who to subscribe to on YT for ML?\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\", \"content\": \"Subscribe to\"\n",
        "    }\n",
        "]\n",
        "target_response = \"Neural Breaddown with AVB\""
      ],
      "metadata": {
        "id": "keZMgCP2AGes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokenized = tokenizer.apply_chat_template(training_prompt, continue_final_message=True, return_tensors=\"pt\").to(device)\n",
        "test_out = model.generate(test_tokenized, max_new_tokens=10)\n",
        "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "cdF3sQmqGw91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "FZfkSX-ddiSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_input_output_pair(prompt=[training_prompt], target_responses=[target_response])\n",
        "print(data)"
      ],
      "metadata": {
        "id": "7oPLE4eFetV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_input_output_pair_test(prompt=[training_prompt], target_responses=[target_response])\n",
        "data[\"input_ids\"] = data[\"input_ids\"].to(device)\n",
        "data[\"labels\"] = data[\"labels\"].to(device)\n",
        "\n",
        "# optimizer update the weight of the network after the drop loss.\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "for i in range(10):\n",
        "  out = model(input_ids=data[\"input_ids\"].to(device))\n",
        "  loss = calculate_loss(out.logits, data[\"labels\"]).mean()\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "ecUvtRQAINiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n",
        "print(\"-------------------------\")\n",
        "print(data[\"input_ids\"])\n",
        "print(data[\"input_ids\"].shape)\n",
        "print(\"-------------------------\")\n",
        "print(data[\"labels\"])\n",
        "print(data[\"labels\"].shape)\n",
        "print(\"-------------------------\")\n",
        "print(data[\"attention_mask\"])\n",
        "print(data[\"attention_mask\"].shape)\n",
        "print(\"-------------------------\")"
      ],
      "metadata": {
        "id": "zKcCg4on-OVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LORA\n"
      ],
      "metadata": {
        "id": "flgkQjQ2Vocq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if hasattr(model, 'peft_config'):\n",
        "    model.unload()"
      ],
      "metadata": {
        "id": "1RmKysVtbfC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "tXqVtvXHAchW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J2pgxkEZhvde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}